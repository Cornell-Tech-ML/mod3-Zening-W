(.venv) PS C:\Users\NING0\mle\mod3-Zening-W> python project/parallel_check.py
MAP
 
================================================================================
 Parallel Accelerator Optimizing:  Function tensor_map.<locals>._map, 
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (163)  
================================================================================


Parallel loop listing for  Function tensor_map.<locals>._map, C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (163) 
-----------------------------------------------------------------------------------------------|loop #ID
    def _map(                                                                                  | 
        out: Storage,                                                                          | 
        out_shape: Shape,                                                                      | 
        out_strides: Strides,                                                                  | 
        in_storage: Storage,                                                                   | 
        in_shape: Shape,                                                                       | 
        in_strides: Strides,                                                                   | 
    ) -> None:                                                                                 | 
        # TODO: Implement for Task 3.1.                                                        | 
        if np.array_equal(out_shape, in_shape) and np.array_equal(out_strides, in_strides):    | 
            for i in prange(len(out)):---------------------------------------------------------| #2
                out[i] = fn(in_storage[i])                                                     | 
            return                                                                             | 
        # Calculate the total size of the tensor                                               | 
        size = len(out)                                                                        | 
                                                                                               | 
        # Parallelize the main loop using prange                                               | 
        for i in prange(size):-----------------------------------------------------------------| #3
            # Convert linear index i to tensor index                                           |
            out_index = np.zeros(MAX_DIMS, np.int32)-------------------------------------------| #0
            to_index(i, out_shape, out_index)                                                  |
                                                                                               |
            # Map output index to input index (for broadcasting)                               |
            in_index = np.zeros(MAX_DIMS, np.int32)--------------------------------------------| #1
            broadcast_index(out_index, out_shape, in_shape, in_index)                          |
                                                                                               |
            # Calculate positions in storage                                                   |
            out_pos = index_to_position(out_index, out_strides)                                |
            in_pos = index_to_position(in_index, in_strides)                                   |
                                                                                               |
            # Apply the function and store result                                              |
            out[out_pos] = fn(in_storage[in_pos])                                              |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 3 parallel for-
loop(s) (originating from loops labelled: #2, #3, #0).
--------------------------------------------------------------------------------
---------------------------- Optimising loop nests -----------------------------
Attempting loop nest rewrites (optimising for the largest parallel loops)...

+--3 is a parallel loop
   +--0 --> rewritten as a serial loop
   +--1 --> rewritten as a serial loop
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
Parallel region 0:
+--3 (parallel)
   +--0 (parallel)
   +--1 (parallel)


--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel region 0:
+--3 (parallel)
   +--0 (serial)
   +--1 (serial)



Parallel region 0 (loop #3) had 0 loop(s) fused and 2 loop(s) serialized as part
 of the larger parallel loop (#3).
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (186) is hoisted out of
the parallel loop labelled #3 (it will be performed before the loop is executed
and reused inside the loop):
   Allocation:: in_index = np.zeros(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (182) is hoisted out of
the parallel loop labelled #3 (it will be performed before the loop is executed
and reused inside the loop):
   Allocation:: out_index = np.zeros(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
None
ZIP
 
================================================================================
 Parallel Accelerator Optimizing:  Function tensor_zip.<locals>._zip,
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (222)
================================================================================


Parallel loop listing for  Function tensor_zip.<locals>._zip, C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (222)
-----------------------------------------------------------------------------|loop #ID
    def _zip(                                                                |
        out: Storage,                                                        |
        out_shape: Shape,                                                    |
        out_strides: Strides,                                                |
        a_storage: Storage,                                                  |
        a_shape: Shape,                                                      |
        a_strides: Strides,                                                  |
        b_storage: Storage,                                                  |
        b_shape: Shape,                                                      |
        b_strides: Strides,                                                  |
    ) -> None:                                                               |
        if (                                                                 |
            np.array_equal(out_shape, a_shape)                               |
            and np.array_equal(out_shape, b_shape)                           |
            and np.array_equal(out_strides, a_strides)                       |
            and np.array_equal(out_strides, b_strides)                       |
        ):                                                                   |
            for out_ord in prange(len(out)):---------------------------------| #7
                out[out_ord] = fn(a_storage[out_ord], b_storage[out_ord])    |
            return                                                           |
    # Calculate total size of output tensor                                  |
        size = len(out)                                                      |
                                                                             |
        # Parallelize the main loop using prange                             |
        for i in prange(size):-----------------------------------------------| #8
            # Convert linear index to tensor index                           |
            out_index = np.zeros(MAX_DIMS, np.int32)-------------------------| #4
            to_index(i, out_shape, out_index)                                |
                                                                             |
            # Map output index to input indices (for broadcasting)           |
            a_index = np.zeros(MAX_DIMS, np.int32)---------------------------| #5
            b_index = np.zeros(MAX_DIMS, np.int32)---------------------------| #6
            broadcast_index(out_index, out_shape, a_shape, a_index)          |
            broadcast_index(out_index, out_shape, b_shape, b_index)          |
                                                                             |
            # Calculate positions in storage                                 |
            out_pos = index_to_position(out_index, out_strides)              |
            a_pos = index_to_position(a_index, a_strides)                    |
            b_pos = index_to_position(b_index, b_strides)                    |
                                                                             |
            # Apply binary function to inputs and store result               |
            out[out_pos] = fn(a_storage[a_pos], b_storage[b_pos])            |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...

Fused loop summary:
+--5 has the following loops fused into it:
   +--6 (fused)
Following the attempted fusion of parallel for-loops there are 3 parallel for-
loop(s) (originating from loops labelled: #7, #8, #4).
--------------------------------------------------------------------------------
---------------------------- Optimising loop nests -----------------------------
Attempting loop nest rewrites (optimising for the largest parallel loops)...

+--8 is a parallel loop
   +--4 --> rewritten as a serial loop
   +--5 --> rewritten as a serial loop
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
Parallel region 0:
+--8 (parallel)
   +--4 (parallel)
   +--5 (parallel)
   +--6 (parallel)


--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel region 0:
+--8 (parallel)
   +--4 (serial)
   +--5 (serial, fused with loop(s): 6)



Parallel region 0 (loop #8) had 1 loop(s) fused and 2 loop(s) serialized as part
 of the larger parallel loop (#8).
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (248) is hoisted out of
the parallel loop labelled #8 (it will be performed before the loop is executed
and reused inside the loop):
   Allocation:: out_index = np.zeros(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (252) is hoisted out of
the parallel loop labelled #8 (it will be performed before the loop is executed
and reused inside the loop):
   Allocation:: a_index = np.zeros(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (253) is hoisted out of
the parallel loop labelled #8 (it will be performed before the loop is executed
and reused inside the loop):
   Allocation:: b_index = np.zeros(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
None
REDUCE
 
================================================================================
 Parallel Accelerator Optimizing:  Function tensor_reduce.<locals>._reduce,
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (289)
================================================================================


Parallel loop listing for  Function tensor_reduce.<locals>._reduce, C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (289)
---------------------------------------------------------------------|loop #ID
    def _reduce(                                                     |
        out: Storage,                                                |
        out_shape: Shape,                                            |
        out_strides: Strides,                                        |
        a_storage: Storage,                                          |
        a_shape: Shape,                                              |
        a_strides: Strides,                                          |
        reduce_dim: int,                                             |
    ) -> None:                                                       |
   # Calculate the size of the output tensor                         |
        size = len(out)                                              |
        # Get the size of the dimension being reduced                |
        reduce_size = a_shape[reduce_dim]                            |
                                                                     |
        # Parallelize the main loop using prange                     |
        for i in prange(size):---------------------------------------| #11
            # Convert linear index to tensor index                   |
            out_index = np.zeros(MAX_DIMS, np.int32)-----------------| #9
            to_index(i, out_shape, out_index)                        |
                                                                     |
            # Create index for accessing the input tensor            |
            a_index = np.zeros(len(a_shape), np.int32)---------------| #10
            for j in range(len(out_shape)):                          |
                a_index[j] = out_index[j]                            |
                                                                     |
            # Get the output position                                |
            out_pos = index_to_position(out_index, out_strides)      |
                                                                     |
            # Perform reduction along the specified dimension        |
            for j in range(reduce_size):                             |
                a_index[reduce_dim] = j                              |
                a_pos = index_to_position(a_index, a_strides)        |
                out[out_pos] = fn(out[out_pos], a_storage[a_pos])    |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 2 parallel for-
loop(s) (originating from loops labelled: #11, #9).
--------------------------------------------------------------------------------
---------------------------- Optimising loop nests -----------------------------
Attempting loop nest rewrites (optimising for the largest parallel loops)...

+--11 is a parallel loop
   +--9 --> rewritten as a serial loop
   +--10 --> rewritten as a serial loop
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
Parallel region 0:
+--11 (parallel)
   +--9 (parallel)
   +--10 (parallel)


--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel region 0:
+--11 (parallel)
   +--9 (serial)
   +--10 (serial)



Parallel region 0 (loop #11) had 0 loop(s) fused and 2 loop(s) serialized as
part of the larger parallel loop (#11).
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (306) is hoisted out of
the parallel loop labelled #11 (it will be performed before the loop is executed
 and reused inside the loop):
   Allocation:: out_index = np.zeros(MAX_DIMS, np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (310) is hoisted out of
the parallel loop labelled #11 (it will be performed before the loop is executed
 and reused inside the loop):
   Allocation:: a_index = np.zeros(len(a_shape), np.int32)
    - numpy.empty() is used for the allocation.
None
MATRIX MULTIPLY
 
================================================================================
 Parallel Accelerator Optimizing:  Function _tensor_matrix_multiply,
C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (326)
================================================================================


Parallel loop listing for  Function _tensor_matrix_multiply, C:\Users\NING0\mle\mod3-Zening-W\minitorch\fast_ops.py (326)
-------------------------------------------------------------------|loop #ID
def _tensor_matrix_multiply(                                       |
    out: Storage,                                                  |
    out_shape: Shape,                                              |
    out_strides: Strides,                                          |
    a_storage: Storage,                                            |
    a_shape: Shape,                                                |
    a_strides: Strides,                                            |
    b_storage: Storage,                                            |
    b_shape: Shape,                                                |
    b_strides: Strides,                                            |
) -> None:                                                         |
    """NUMBA tensor matrix multiply function.                      |
                                                                   |
    Should work for any tensor shapes that broadcast as long as    |
                                                                   |
    ```                                                            |
    assert a_shape[-1] == b_shape[-2]                              |
    ```                                                            |
                                                                   |
    Optimizations:                                                 |
                                                                   |
    * Outer loop in parallel                                       |
    * No index buffers or function calls                           |
    * Inner loop should have no global writes, 1 multiply.         |
                                                                   |
                                                                   |
    Args:                                                          |
    ----                                                           |
        out (Storage): storage for `out` tensor                    |
        out_shape (Shape): shape for `out` tensor                  |
        out_strides (Strides): strides for `out` tensor            |
        a_storage (Storage): storage for `a` tensor                |
        a_shape (Shape): shape for `a` tensor                      |
        a_strides (Strides): strides for `a` tensor                |
        b_storage (Storage): storage for `b` tensor                |
        b_shape (Shape): shape for `b` tensor                      |
        b_strides (Strides): strides for `b` tensor                |
                                                                   |
    Returns:                                                       |
    -------                                                        |
        None : Fills in `out`                                      |
                                                                   |
    """                                                            |
    a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0         |
    b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0         |
                                                                   |
# Get the dimensions                                               |
    batch_size = out_shape[0]                                      |
    rows = out_shape[1]                                            |
    cols = out_shape[2]                                            |
    reduce_dim = a_shape[2]                                        |
                                                                   |
    # Parallelize the outer loops                                  |
    for batch in prange(batch_size):-------------------------------| #13
        for i in prange(rows):-------------------------------------| #12
            for j in range(cols):                                  |
                # Calculate output position                        |
                out_pos = (                                        |
                    batch * out_strides[0] +                       |
                    i * out_strides[1] +                           |
                    j * out_strides[2]                             |
                )                                                  |
                                                                   |
                # Initialize accumulator                           |
                acc = 0.0                                          |
                                                                   |
                # Inner reduction loop                             |
                for k in range(reduce_dim):                        |
                    # Calculate positions in a and b tensors       |
                    a_pos = (                                      |
                        batch * a_batch_stride +                   |
                        i * a_strides[1] +                         |
                        k * a_strides[2]                           |
                    )                                              |
                    b_pos = (                                      |
                        batch * b_batch_stride +                   |
                        k * b_strides[1] +                         |
                        j * b_strides[2]                           |
                    )                                              |
                    # Multiply and accumulate                      |
                    acc += a_storage[a_pos] * b_storage[b_pos]     |
                                                                   |
                # Store final result                               |
                out[out_pos] = acc                                 |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 2 parallel for-
loop(s) (originating from loops labelled: #13, #12).
--------------------------------------------------------------------------------
---------------------------- Optimising loop nests -----------------------------
Attempting loop nest rewrites (optimising for the largest parallel loops)...

+--13 is a parallel loop
   +--12 --> rewritten as a serial loop
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
Parallel region 0:
+--13 (parallel)
   +--12 (parallel)


--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel region 0:
+--13 (parallel)
   +--12 (serial)



Parallel region 0 (loop #13) had 0 loop(s) fused and 1 loop(s) serialized as
part of the larger parallel loop (#13).
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None